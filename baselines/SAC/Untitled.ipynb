{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on CPU, GPU is too old\n",
      "Target Entropy -3\n",
      "Initial Exploration ...\n",
      "Finished Initial Exploration on 2500 steps \n",
      "\n",
      "Finished Epoch 0, replay size 7500\n",
      "Finished Epoch 1, replay size 12500\n",
      "Finished Epoch 2, replay size 17500\n",
      "Finished Epoch 3, replay size 22500\n",
      "Finished Epoch 4, replay size 27500\n",
      "\n",
      " MADE IT 1 TIMES TO GOAL \n",
      "\n",
      "Finished Epoch 5, replay size 32500\n",
      "\n",
      " MADE IT 2 TIMES TO GOAL \n",
      "\n",
      "Finished Epoch 6, replay size 37500\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import argparse\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.insert(0,'../../envs/')\n",
    "sys.path.insert(0,'../Core/')\n",
    "import os\n",
    "from utils import *\n",
    "from global_vars import BATCH_SIZE, DT, SEED\n",
    "from PegRobot2D import Frontend, WINDOW_X, WINDOW_Y\n",
    "import numpy as np\n",
    "import torch\n",
    "from sac import SAC\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from rl_batch_trainer import BatchRLAlgorithm\n",
    "\n",
    "save_dir = \"models/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "actor_path = save_dir\n",
    "if not os.path.exists(actor_path):\n",
    "    os.makedirs(actor_path)\n",
    "critic_path = save_dir\n",
    "if not os.path.exists(critic_path):\n",
    "    os.makedirs(critic_path)\n",
    "\n",
    "variant = dict(\n",
    "        algorithm=\"SAC\",\n",
    "        version=\"normal\",\n",
    "        seed = 0,\n",
    "        replay_buffer_size=int(1e5),\n",
    "        save_model = True,\n",
    "        algorithm_kwargs=dict(\n",
    "            num_epochs= 25,\n",
    "            num_eval_steps_per_epoch= 2500,\n",
    "            num_train_loops_per_epoch = 5,\n",
    "            num_trains_per_train_loop= 100,\n",
    "            num_expl_steps_per_train_loop = 1000,\n",
    "            min_num_steps_before_training = 2500, # Random exploration steps Initially\n",
    "            max_path_length=500,\n",
    "            batch_size=256,\n",
    "        ),\n",
    "    \n",
    "        trainer_kwargs=dict(\n",
    "            gamma=0.99,\n",
    "            tau=5e-3,\n",
    "            target_update_interval=1,\n",
    "            lr=3e-4,\n",
    "            alpha = 0.2,\n",
    "            policy = \"Gaussian\",\n",
    "            automatic_entropy_tuning=True,\n",
    "            hidden_size = 256\n",
    "        ),\n",
    "    \n",
    "        env_args = [\n",
    "            WINDOW_X,\n",
    "            WINDOW_Y,\n",
    "            \"Peg 2D Robot\"\n",
    "            ],\n",
    "    \n",
    "        env_kwargs = dict(\n",
    "            vsync = False,\n",
    "            resizable = False,\n",
    "            visible = False\n",
    "            )\n",
    ")\n",
    "\n",
    "# Environment\n",
    "env = Frontend(*variant['env_args'], **variant['env_kwargs'])\n",
    "if variant['algorithm'] is \"SAC\":\n",
    "    env.denorm_process = False # No need to denorm because in SAC the gaussian policies are already scaled up\n",
    "    \n",
    "torch.manual_seed(variant['seed'])\n",
    "np.random.seed(variant['seed'])\n",
    "\n",
    "# Agent\n",
    "num_actions = env.num_actions\n",
    "num_inputs = env.num_states\n",
    "action_range = env.action_range\n",
    "\n",
    "agent = SAC(num_inputs, num_actions, action_range, **variant['trainer_kwargs'])\n",
    "\n",
    "# Tensorboard\n",
    "log_dir = 'runs/{}_SAC_{}_{}_{}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "                                        \"Peg2DRobot\",\n",
    "                                        variant['trainer_kwargs']['policy'],\n",
    "                                       \"autotune\" if variant['trainer_kwargs']['automatic_entropy_tuning'] else \"\")\n",
    "writer = SummaryWriter(logdir=log_dir)\n",
    "\n",
    "# Replay Memory\n",
    "replay_buffer = ReplayBuffer(variant['replay_buffer_size'])\n",
    "\n",
    "# ####################\n",
    "# # MAIN training loop\n",
    "# ####################\n",
    "RL_trainer = BatchRLAlgorithm(replay_buffer, **variant['algorithm_kwargs']) \n",
    "RL_trainer.train(env, agent, writer)\n",
    "\n",
    "if variant['save_model']:\n",
    "    agent.save_model(\"Peg2D\")\n",
    "# # PER is a good idea for SAC because once we get BIG erwards... the critic is very unstable, \n",
    "# # so values with high TD error are definately more important\n",
    "# # Normalised states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 256\n",
      "max_path_length = 250\n",
      "num_epochs = 50\n",
      "num_eval_steps_per_epoch = 1000\n",
      "num_trains_per_train_loop = 250\n",
      "num_train_loops_per_epoch = 1\n",
      "num_expl_steps_per_train_loop = 1000\n",
      "min_num_steps_before_training = 2500\n",
      "path_type = ['random', 'exploration', 'eval']\n",
      "replay_buffer = <utils.ReplayBuffer object at 0x000002A7C788D080>\n",
      "memory_steps = 52500\n",
      "updates = 12500\n"
     ]
    }
   ],
   "source": [
    "for attribute, value in RL_trainer.__dict__.items():\n",
    "    print(attribute, '=', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on CPU, GPU is too old\n",
      "Target Entropy -3\n",
      "Loading models from models/actor_Peg2D_ and models/critic_Peg2D_\n",
      "1058 371\n",
      "0.5140625\n",
      "Vec2d(1149.9917398551534, 363.04695559348136)\n",
      "<pymunk.shapes.Poly object at 0x0000018BA00E9C18> Body(6074.15926535898, 20279593.06727851, Body.DYNAMIC)\n",
      "959 372\n",
      "0.43671875\n",
      "Vec2d(1149.990763005496, 362.36617360457024)\n",
      "<pymunk.shapes.Poly object at 0x0000018BA00E9C18> Body(6074.15926535898, 20279593.06727851, Body.DYNAMIC)\n"
     ]
    }
   ],
   "source": [
    "from sac import SAC\n",
    "from utils import *\n",
    "from models import weights_init\n",
    "import sys\n",
    "import argparse\n",
    "sys.path.insert(0,'../../envs/')\n",
    "from PegRobot2D import Frontend, WINDOW_X, WINDOW_Y\n",
    "\n",
    "variant = dict(\n",
    "        algorithm=\"SAC\",\n",
    "        version=\"normal\",\n",
    "        seed = 123456,\n",
    "        replay_buffer_size=int(1e5),\n",
    "        save_model = True,\n",
    "        algorithm_kwargs=dict(\n",
    "            num_epochs=50,\n",
    "            num_eval_steps_per_epoch=1000,\n",
    "            num_trains_per_train_loop=250,\n",
    "            num_expl_steps_per_train_loop=1000,\n",
    "            min_num_steps_before_training=2500, # Random exploration steps Initially\n",
    "            max_path_length=250,\n",
    "            batch_size=256,\n",
    "        ),\n",
    "    \n",
    "        trainer_kwargs=dict(\n",
    "            gamma=0.99,\n",
    "            tau=5e-3,\n",
    "            target_update_interval=1,\n",
    "            lr=3e-4,\n",
    "            alpha = 0.2,\n",
    "            policy = \"Gaussian\",\n",
    "            automatic_entropy_tuning=True,\n",
    "            hidden_size = 256\n",
    "        ),\n",
    "    \n",
    "        env_args = [\n",
    "            WINDOW_X,\n",
    "            WINDOW_Y,\n",
    "            \"Peg 2D Robot\"\n",
    "            ],\n",
    "    \n",
    "        env_kwargs = dict(\n",
    "            vsync = False,\n",
    "            resizable = False,\n",
    "            visible = False\n",
    "            )\n",
    ")\n",
    "\n",
    "# Environment\n",
    "env = Frontend(*variant['env_args'], **variant['env_kwargs'])\n",
    "if variant['algorithm'] is \"SAC\":\n",
    "    env.denorm_process = False # No need to denorm because in SAC the gaussian policies are already scaled up\n",
    "    \n",
    "torch.manual_seed(variant['seed'])\n",
    "np.random.seed(variant['seed'])\n",
    "\n",
    "def run_policy(agent, env = None, framework = \"SAC\"):\n",
    "    if isinstance(env, Frontend):\n",
    "        del(env)\n",
    "    env = Frontend(WINDOW_X, WINDOW_Y, \"RoboPeg2D Simulation\", vsync = False, resizable = False, visible = True)\n",
    "    env.agent = agent\n",
    "    if framework is \"SAC\":\n",
    "        env.denorm_process = False # Necessary for SAC\n",
    "    env.run_policy(agent)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Frontend(WINDOW_X, WINDOW_Y, \"RoboPeg2D Simulation\", vsync = False, resizable = False, visible = False)\n",
    "    \n",
    "    # Agent\n",
    "    num_actions = env.num_actions\n",
    "    num_inputs = env.num_states\n",
    "    action_range = env.action_range\n",
    "\n",
    "    tst_agent = SAC(num_inputs, num_actions, action_range, **variant['trainer_kwargs'])\n",
    "\n",
    "    tst_agent.load_model(actor_path=\"models/actor_Peg2D_\",critic_path=\"models/critic_Peg2D_\")\n",
    "\n",
    "    run_policy(tst_agent, env, \"SAC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
